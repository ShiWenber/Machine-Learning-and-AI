{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>150</th>\n",
       "      <th>4</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     150    4  setosa  versicolor  virginica\n",
       "0    5.1  3.5     1.4         0.2          0\n",
       "1    4.9  3.0     1.4         0.2          0\n",
       "2    4.7  3.2     1.3         0.2          0\n",
       "3    4.6  3.1     1.5         0.2          0\n",
       "4    5.0  3.6     1.4         0.2          0\n",
       "..   ...  ...     ...         ...        ...\n",
       "145  6.7  3.0     5.2         2.3          2\n",
       "146  6.3  2.5     5.0         1.9          2\n",
       "147  6.5  3.0     5.2         2.0          2\n",
       "148  6.2  3.4     5.4         2.3          2\n",
       "149  5.9  3.0     5.1         1.8          2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/iris.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[5.1 3.5 1.4 0.2 0. ]\n",
      "(150, 5)\n",
      "(150, 3)\n",
      "[-2.68412563  0.31939725  0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = load_iris()\n",
    "print(type(iris))\n",
    "X = iris.data # 四维特征\n",
    "y = iris.target # 标签（类）\n",
    "\n",
    "# 将X, y水平拼接 y.reshape(-1, 1) 将y转换为列向量(y原来是行向量(150,))\n",
    "X_y = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# print(X)\n",
    "print(y)\n",
    "print(X_y[0])\n",
    "print(X_y.shape)\n",
    "\n",
    "# 对数据做pca降维指定维度\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# 对X做pca降维\n",
    "X_pca = pca.fit_transform(X)\n",
    "# 将X_pca, y水平拼接\n",
    "X_pca_y = np.concatenate((X_pca, y.reshape(-1, 1)), axis=1)\n",
    "print(X_pca_y.shape)\n",
    "print(X_pca_y[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对四个特征pca降维为2后再使用两个权重参数一个bias来拟合数据，最终结果采用pca降低特征维度后展示\n",
    "\n",
    "使用交叉验证，将所有数据集划分为5份，其中4份用于训练，1份用于测试，轮换后最终得到5个测试结果，取平均值作为最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 3)\n",
      "(120,)\n"
     ]
    }
   ],
   "source": [
    "# %pdb # 触发assert报错时，自动进入调试模式\n",
    "# dataset \n",
    "## 读取sklearn自带的数据集，变量类型为sk.utils.Bunch\n",
    "class Dataset:\n",
    "    def __init__(self, data: sk.utils.Bunch):\n",
    "        \n",
    "        \n",
    "        # 对数据做pca降维指定维度\n",
    "        pca = PCA(n_components=2)\n",
    "        X = data.data # 四维特征\n",
    "        y = data.target # 标签（类）\n",
    "        \n",
    "        # 对X做pca降维\n",
    "        X_pca = pca.fit_transform(X)\n",
    "\n",
    "        # 将data.data和data.target随机打乱切分为五份，每份的比例为0.2，并将data.data和data.target水平拼接\n",
    "        temp_data = np.concatenate((X_pca, y.reshape(-1, 1)), axis=1)\n",
    "        random_data = np.random.permutation(temp_data) # 将data.data随机打乱\n",
    "        # random_data插入倒数第二列，插入的值为1，用于bias\n",
    "        random_data = np.insert(random_data, random_data.shape[1]-1, 1, axis=1)\n",
    "        assert random_data.shape == (150, 4)\n",
    "        assert random_data[0][-2] == 1\n",
    "        self.data = random_data # 150*4\n",
    "        # 将数据集切分为五份，每份的比例为0.2\n",
    "        self.train_idx = int(0.8 * data.data.shape[0])\n",
    "        self.train_feature = self.data[0:self.train_idx, 0:-1]\n",
    "        assert self.train_feature.shape == (120, 3)\n",
    "        self.train_label = self.data[0:self.train_idx, -1]\n",
    "        assert self.train_label.shape == (120,)\n",
    "        self.test_feature = self.data[self.train_idx:, 0:-1]\n",
    "        assert self.test_feature.shape == (30, 3)\n",
    "        self.test_label = self.data[self.train_idx:, -1]\n",
    "        assert self.test_label.shape == (30,)\n",
    "    def random(self):\n",
    "        self.data = np.random.permutation(self.data)\n",
    "        self.train_feature = self.data[0:self.train_idx, 0:-1]\n",
    "        self.train_label = self.data[0:self.train_idx, -1]\n",
    "dataset = Dataset(iris)\n",
    "print(dataset.train_feature.shape)\n",
    "print(dataset.train_label.shape)\n",
    "del dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax regression\n",
    "\n",
    "反向传播的公式：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial{L( \\mathbf{W} )}}{\\partial{u_j}} = \\begin{cases}\n",
    "    \\frac{\\exp{(u_j)}}{\\sum_{j=1}^k \\exp^{u_j}}, j* \\neq j \\\\\n",
    "    \\frac{\\exp{u_j} }{\\sum_{j=1}^k \\exp ^{u_j}} - 1, j* = j\n",
    "\\end{cases} \\\\\n",
    "&u_j = \\mathbf{w_j}^T \\mathbf{x} + b_j \\\\\n",
    "&u_{j*} = \\mathbf{w_{j*}}^T \\mathbf{x} + b_{j*} \\\\\n",
    "&\\text{具体到每个参数的计算：} \\\\\n",
    "&\\frac{\\partial L(\\mathbf{W})}{\\partial w_{jl}} = \\begin{cases}\n",
    "    y_jx_l, j*\\neq j\\\\\n",
    "    (y_j - 1)x_l, j* = j\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "随机梯度下降和小批量梯度下降的区别：\n",
    "- 小批量梯度下降在每个batch上计算各条数据所得梯度的平均值来更新参数\n",
    "- 随机梯度下降每条数据都会更新参数\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/72929546"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    def __init__(self, num_features, num_class):\n",
    "        # 随机初始化参数 num_features * num_class num_feature 包含了bias\n",
    "        ## W num_features * num_class # 这里也需要加入bias，bias = W[-1, :] 1 * num_class\n",
    "        self.num_features = num_features\n",
    "        self.num_class = num_class\n",
    "        self.w = np.random.randn(num_features, num_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播返回yhat_p，x的数据格式为(num_features) * n，当使用backward时，一次只能传入一组数据，但是当计算loss时，可以一次传入多组数据做前向传播\n",
    "\n",
    "        Args:\n",
    "            x (np.array): 为一组数据，格式为 n * num_features ，应当转置为(num_features ) * n\n",
    "\n",
    "        Returns:\n",
    "            yhat_p (np.array): 类别概率分布 num_class * n\n",
    "        \"\"\"\n",
    "        x = x.T # (num_features ) * n\n",
    "        assert x.shape[0] == self.num_features\n",
    "        # W^T * x = [z1, z2, z3]^T\n",
    "        temp = np.matmul(self.w.T, x) # num_class * n\n",
    "        # y_k = p(y = k | x) = exp(z_k) / sum(exp(z_i))\n",
    "        assert temp.shape == (self.num_class, x.shape[1])\n",
    "        # 对temp列求和\n",
    "        sum = np.sum(np.exp(temp), axis=0) # 1 * n\n",
    "        print(sum.shape)\n",
    "        # 将sum 转化为 1 * n\n",
    "        sum = sum.reshape(1, -1)\n",
    "        assert sum.shape == (1, x.shape[1])\n",
    "        sum_m = np.repeat(sum, self.num_class, axis=0) # num_class * n\n",
    "        assert sum_m.shape == (self.num_class, x.shape[1])\n",
    "        # yhat_p = np.exp(temp) / sum # num_class * 1\n",
    "        yhat_p = np.divide(np.exp(temp), sum_m) # num_class * n\n",
    "        assert yhat_p.shape == (self.num_class, x.shape[1])\n",
    "        return yhat_p\n",
    "    \n",
    "    def backward(self, x, yhat_p, y, lr):\n",
    "        \"\"\"反向传播，直接更新参数，返回负梯度只是作为log，使用随机梯度下降法\n",
    "\n",
    "        Args:\n",
    "            x (输入特征): 输入的一条数据,(num_features) * 1\n",
    "            yhat_p (预测概率分布): num_class * 1\n",
    "            y (实际标签值): 1 * 1\n",
    "        Returns:\n",
    "            grad (np.array): 负梯度矩阵 num_class * (num_features)\n",
    "        \"\"\"\n",
    "        yhat = np.argmax(yhat_p) # 获取每个样本的预测类别 1 * 1\n",
    "        # 构建梯度矩阵 grad.shape = (num_class, num_features)\n",
    "        grad = np.zeros((self.num_class, self.num_features))\n",
    "        for j in range(self.num_class):\n",
    "            for l in range(self.num_features):\n",
    "                if yhat == y:\n",
    "                    grad[j][l] = (y - 1) * x[l]\n",
    "                else:\n",
    "                    grad[j][l] = y * x[l]\n",
    "        # backward内部更新参数一下是随机梯度更新方式（一次只取一条数据）\n",
    "        self.w = self.w - lr * grad\n",
    "        return -grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss \n",
    "def loss(model, x, y):\n",
    "    \"\"\"计算loss\n",
    "\n",
    "    Args:\n",
    "        model (SoftmaxRegression): 模型主要取w\n",
    "        x (np.array): 输入特征 n * num_features\n",
    "        y (np.array): 1 * n\n",
    "    \n",
    "    Returns:\n",
    "        loss (np.array): 1 * 1\n",
    "    \"\"\"\n",
    "    print(x.shape)\n",
    "    y = y.astype(int)\n",
    "    sum_loss = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        sum = 0\n",
    "        assert x[i].shape   == (x.shape[1],) # (num_features,)\n",
    "        print(model.w.shape)\n",
    "        print(model.w[:, y].shape)\n",
    "        assert model.w[:, y].shape == (x.shape[1], 1) # num_features * 1\n",
    "        u_j_act = np.matmul(model.w[:, y].T , x[i])\n",
    "        assert u_j_act.shape == (1,) # 1 * 1\n",
    "        print(u_j_act.shape)\n",
    "        # assert 应该为一个数\n",
    "        assert u_j_act.shape == (1,)\n",
    "        print(u_j_act.shape)\n",
    "        for j in range(x.shape[1]):\n",
    "            u_j = np.matmul(model.w[:, j], x[i])\n",
    "            sum += np.log(u_j) - u_j_act\n",
    "        loss = np.log(sum)\n",
    "        sum_loss += loss\n",
    "    \n",
    "    return sum_loss/ x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "def train(model, dataset: Dataset, num_epochs, learning_rate = 0.01, random = True):\n",
    "    file = None\n",
    "    if random:\n",
    "        file = open(f\"log2/log_e{num_epochs}_lr{learning_rate}_random.csv\", \"w\")\n",
    "    else:\n",
    "        file = open(f\"log2/log_e{num_epochs}_lr{learning_rate}.csv\", \"w\")\n",
    "    file.write(f\"epoch,loss,test_loss(cost),wight\\n\")\n",
    "    cost = -1\n",
    "    for e in range(num_epochs):\n",
    "        if random:\n",
    "            dataset.random()\n",
    "            #### Compute outputs #### \n",
    "            yhat = model.forward(dataset.train_feature)\n",
    "            num_features = dataset.train_feature.shape[1]\n",
    "            num_data = dataset.train_feature.shape[0]\n",
    "            assert yhat.shape == (model.num_class, num_data) # num_class * n\n",
    "            #### Compute gradients and Update gradients ####\n",
    "            for i in range(num_data):\n",
    "                negative_grad_tmp = model.backward(dataset.train_feature[i], yhat[:, i], dataset.train_label[i], learning_rate)\n",
    "            #### Logging ####\n",
    "            # 测试集上平均均方误差 \n",
    "            cost = loss(model, dataset.test_feature, dataset.test_label)\n",
    "        file.write(f\"{e}, {loss(model, dataset.train_feature, dataset.train_label)}, {cost}, {model.w}\\n\")\n",
    "        print(f\"epoch:{e} | loss:{loss(model,dataset.train_feature, dataset.train_label)} | test_loss(cost):{cost}\")\n",
    "    file.close()\n",
    "    if random:\n",
    "        print(f\"log_file: log_e{num_epochs}_lr{learning_rate}_random.csv\")\n",
    "    else:\n",
    "        print(f\"log_file: log_e{num_epochs}_lr{learning_rate}.csv\")\n",
    "    return cost   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.86624165  0.06936447  1.        ]\n",
      " [-2.20948924  0.43666314  1.        ]\n",
      " [-2.56231991  0.36771886  1.        ]\n",
      " [ 1.04413183  0.2283619   1.        ]\n",
      " [-2.59873675  1.09314576  1.        ]\n",
      " [ 2.38800302  0.4646398   1.        ]\n",
      " [ 0.98493451 -0.12481785  1.        ]\n",
      " [ 2.91675097  0.78279195  1.        ]\n",
      " [ 1.55780216  0.26749545  1.        ]\n",
      " [-0.0087454  -0.72308191  1.        ]\n",
      " [ 0.81329065 -0.1633503   1.        ]\n",
      " [ 2.31415471  0.18365128  1.        ]\n",
      " [ 3.23067366  1.37416509  1.        ]\n",
      " [ 0.24595768 -0.2685244   1.        ]\n",
      " [-2.63692688 -0.12132235  1.        ]\n",
      " [-2.40561449  0.18887143  1.        ]\n",
      " [-2.72871654  0.32675451  1.        ]\n",
      " [-2.71445143 -0.2502082   1.        ]\n",
      " [ 1.41523588 -0.57491635  1.        ]\n",
      " [-2.35575405 -0.03728186  1.        ]\n",
      " [-0.74912267 -1.00489096  1.        ]\n",
      " [ 1.94968906  0.04194326  1.        ]\n",
      " [-2.71414169 -0.17700123  1.        ]\n",
      " [ 1.22069088  0.40761959  1.        ]\n",
      " [ 2.41874618  0.3047982   1.        ]\n",
      " [-2.78610927 -0.235112    1.        ]\n",
      " [ 2.10761114  0.37228787  1.        ]\n",
      " [ 0.46480029 -0.67071154  1.        ]\n",
      " [ 0.58800644 -0.48428742  1.        ]\n",
      " [ 0.29900084 -0.34889781  1.        ]\n",
      " [-2.84936871 -0.94096057  1.        ]\n",
      " [ 2.15943764 -0.21727758  1.        ]\n",
      " [ 1.90509815  0.04930053  1.        ]\n",
      " [ 2.56301338  0.2778626   1.        ]\n",
      " [ 0.66028376 -0.35296967  1.        ]\n",
      " [-2.64886233  0.81336382  1.        ]\n",
      " [-2.82053775 -0.08946138  1.        ]\n",
      " [-2.53814826  0.50377114  1.        ]\n",
      " [-2.88899057 -0.14494943  1.        ]\n",
      " [-2.58739848 -0.20431849  1.        ]\n",
      " [ 0.33193448 -0.21265468  1.        ]\n",
      " [-2.50694709  0.6450689   1.        ]\n",
      " [-3.22380374 -0.51139459  1.        ]\n",
      " [ 2.93258707  0.3555      1.        ]\n",
      " [ 1.25850816 -0.17970479  1.        ]\n",
      " [ 2.61409047  0.56090136  1.        ]\n",
      " [ 1.19900111 -0.60609153  1.        ]\n",
      " [ 1.16932634 -0.16499026  1.        ]\n",
      " [-0.18962247 -0.68028676  1.        ]\n",
      " [ 2.61667602  0.34390315  1.        ]\n",
      " [ 0.35698149 -0.50491009  1.        ]\n",
      " [ 1.38002644 -0.42095429  1.        ]\n",
      " [ 1.28482569  0.68516047  1.        ]\n",
      " [ 1.97153105 -0.1797279   1.        ]\n",
      " [-2.62523805  0.59937002  1.        ]\n",
      " [-2.68412563  0.31939725  1.        ]\n",
      " [ 2.32122882 -0.2438315   1.        ]\n",
      " [ 2.1655918   0.21627559  1.        ]\n",
      " [ 1.52716661 -0.37531698  1.        ]\n",
      " [ 1.34616358 -0.77681835  1.        ]\n",
      " [-2.19982032  0.87283904  1.        ]\n",
      " [ 3.07649993  0.68808568  1.        ]\n",
      " [-0.30558378 -0.36826219  1.        ]\n",
      " [ 0.51169856 -0.10398124  1.        ]\n",
      " [-2.70335978  0.10770608  1.        ]\n",
      " [ 0.18331772 -0.82795901  1.        ]\n",
      " [ 0.92786078  0.46717949  1.        ]\n",
      " [ 1.90094161  0.11662796  1.        ]\n",
      " [-0.90646986 -0.75609337  1.        ]\n",
      " [ 2.14424331  0.1400642   1.        ]\n",
      " [-2.61275523  0.01472994  1.        ]\n",
      " [ 2.35000592 -0.04026095  1.        ]\n",
      " [-0.17392537 -0.25485421  1.        ]\n",
      " [ 0.80685831  0.19418231  1.        ]\n",
      " [ 2.53119273 -0.00984911  1.        ]\n",
      " [ 1.29113206 -0.11666865  1.        ]\n",
      " [ 1.44416124 -0.14341341  1.        ]\n",
      " [ 0.81509524 -0.37203706  1.        ]\n",
      " [-2.38603903  1.33806233  1.        ]\n",
      " [ 0.71485333  0.14905594  1.        ]\n",
      " [ 1.66177415  0.24222841  1.        ]\n",
      " [ 1.41523588 -0.57491635  1.        ]\n",
      " [ 1.80340195 -0.21563762  1.        ]\n",
      " [ 1.29818388 -0.32778731  1.        ]\n",
      " [-2.6727558  -0.11377425  1.        ]\n",
      " [-2.99740655 -0.34192606  1.        ]\n",
      " [-0.50784088 -1.26597119  1.        ]\n",
      " [-3.21593942  0.13346807  1.        ]\n",
      " [-2.62614497  0.16338496  1.        ]\n",
      " [ 2.2754305   0.33499061  1.        ]\n",
      " [ 3.48705536  1.17573933  1.        ]\n",
      " [-2.64829671  0.31184914  1.        ]\n",
      " [-2.46882007  0.13095149  1.        ]\n",
      " [ 0.23610499 -0.33361077  1.        ]\n",
      " [ 1.90445637  0.11925069  1.        ]\n",
      " [-2.63198939 -0.19696122  1.        ]\n",
      " [-2.77010243  0.26352753  1.        ]\n",
      " [ 2.12360872 -0.20972948  1.        ]\n",
      " [ 0.13642871 -0.31403244  1.        ]\n",
      " [-2.63953472  0.31203998  1.        ]\n",
      " [ 1.94410979  0.1875323   1.        ]\n",
      " [-2.83946217 -0.22794557  1.        ]\n",
      " [ 1.78129481 -0.49990168  1.        ]\n",
      " [ 2.84167278  0.37526917  1.        ]\n",
      " [-2.30273318  0.09870885  1.        ]\n",
      " [-2.54308575  0.57941002  1.        ]\n",
      " [ 3.49992004  0.4606741   1.        ]\n",
      " [ 0.37621565 -0.29321893  1.        ]\n",
      " [ 1.08810326  0.07459068  1.        ]\n",
      " [ 3.79564542  0.25732297  1.        ]\n",
      " [ 0.16641322 -0.68192672  1.        ]\n",
      " [ 0.35788842 -0.06892503  1.        ]\n",
      " [ 2.42781791  0.37819601  1.        ]\n",
      " [ 0.23054802 -0.40438585  1.        ]\n",
      " [ 1.9222678   0.40920347  1.        ]\n",
      " [ 0.64257601  0.01773819  1.        ]\n",
      " [ 1.46430232  0.50426282  1.        ]\n",
      " [ 0.04522698 -0.58383438  1.        ]\n",
      " [ 1.39018886 -0.28266094  1.        ]\n",
      " [-2.59000631  0.22904384  1.        ]]\n",
      "[0. 0. 0. 1. 0. 2. 1. 2. 1. 1. 1. 2. 2. 1. 0. 0. 0. 0. 2. 0. 1. 2. 0. 1.\n",
      " 2. 0. 2. 1. 1. 1. 0. 2. 2. 2. 1. 0. 0. 0. 0. 0. 1. 0. 0. 2. 2. 2. 2. 2.\n",
      " 1. 2. 1. 1. 1. 2. 0. 0. 2. 2. 2. 2. 0. 2. 1. 1. 0. 1. 1. 2. 1. 2. 0. 2.\n",
      " 1. 1. 2. 2. 2. 1. 0. 1. 2. 2. 2. 1. 0. 0. 1. 0. 0. 2. 2. 0. 0. 1. 2. 0.\n",
      " 0. 2. 1. 0. 2. 0. 2. 2. 0. 0. 2. 1. 1. 2. 1. 1. 2. 1. 2. 1. 1. 1. 2. 0.]\n",
      "(120,)\n",
      "(30, 3)\n",
      "(3, 30)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39massert\u001b[39;00m X_train\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m120\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m      9\u001b[0m model \u001b[39m=\u001b[39m SoftmaxRegression(num_features\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], num_class\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m train(model, dataset, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, random\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[179], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, num_epochs, learning_rate, random)\u001b[0m\n\u001b[0;32m     20\u001b[0m         negative_grad_tmp \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbackward(dataset\u001b[39m.\u001b[39mtrain_feature[i], yhat[:, i], dataset\u001b[39m.\u001b[39mtrain_label[i], learning_rate)\n\u001b[0;32m     21\u001b[0m     \u001b[39m#### Logging ####\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39m# 测试集上平均均方误差 \u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     cost \u001b[39m=\u001b[39m loss(model, dataset\u001b[39m.\u001b[39;49mtest_feature, dataset\u001b[39m.\u001b[39;49mtest_label)\n\u001b[0;32m     24\u001b[0m file\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mloss(model,\u001b[39m \u001b[39mdataset\u001b[39m.\u001b[39mtrain_feature,\u001b[39m \u001b[39mdataset\u001b[39m.\u001b[39mtrain_label)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mcost\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mw\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch:\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m | loss:\u001b[39m\u001b[39m{\u001b[39;00mloss(model,dataset\u001b[39m.\u001b[39mtrain_feature,\u001b[39m \u001b[39mdataset\u001b[39m.\u001b[39mtrain_label)\u001b[39m}\u001b[39;00m\u001b[39m | test_loss(cost):\u001b[39m\u001b[39m{\u001b[39;00mcost\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[178], line 20\u001b[0m, in \u001b[0;36mloss\u001b[1;34m(model, x, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39massert\u001b[39;00m x[i]\u001b[39m.\u001b[39mshape   \u001b[39m==\u001b[39m (x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],) \u001b[39m# (num_features,)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mw[:, y]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 20\u001b[0m \u001b[39massert\u001b[39;00m model\u001b[39m.\u001b[39mw[:, y]\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m) \u001b[39m# num_features * 1\u001b[39;00m\n\u001b[0;32m     21\u001b[0m u_j_act \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(model\u001b[39m.\u001b[39mw[:, y]\u001b[39m.\u001b[39mT , x[i])\n\u001b[0;32m     22\u001b[0m \u001b[39massert\u001b[39;00m u_j_act\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m,) \u001b[39m# 1 * 1\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = Dataset(iris)\n",
    "\n",
    "X_train = dataset.train_feature\n",
    "Y_train = dataset.train_label\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "\n",
    "assert X_train.shape == (120, 3)\n",
    "model = SoftmaxRegression(num_features=X_train.shape[1], num_class=3)\n",
    "\n",
    "train(model, dataset, num_epochs=100, learning_rate=0.01, random=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
